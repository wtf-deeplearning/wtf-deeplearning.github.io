<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <title>WTF Deep Learning!!!</title>
  <meta name="generator" content="Haroopad 0.13.1" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <style>div.oembedall-githubrepos{border:1px solid #DDD;border-radius:4px;list-style-type:none;margin:0 0 10px;padding:8px 10px 0;font:13.34px/1.4 helvetica,arial,freesans,clean,sans-serif;width:452px;background-color:#fff}div.oembedall-githubrepos .oembedall-body{background:-moz-linear-gradient(center top,#FAFAFA,#EFEFEF);background:-webkit-gradient(linear,left top,left bottom,from(#FAFAFA),to(#EFEFEF));border-bottom-left-radius:4px;border-bottom-right-radius:4px;border-top:1px solid #EEE;margin-left:-10px;margin-top:8px;padding:5px 10px;width:100%}div.oembedall-githubrepos h3{font-size:14px;margin:0;padding-left:18px;white-space:nowrap}div.oembedall-githubrepos p.oembedall-description{color:#444;font-size:12px;margin:0 0 3px}div.oembedall-githubrepos p.oembedall-updated-at{color:#888;font-size:11px;margin:0}div.oembedall-githubrepos ul.oembedall-repo-stats{border:none;float:right;font-size:11px;font-weight:700;padding-left:15px;position:relative;z-index:5;margin:0}div.oembedall-githubrepos ul.oembedall-repo-stats li{border:none;color:#666;display:inline-block;list-style-type:none;margin:0!important}div.oembedall-githubrepos ul.oembedall-repo-stats li a{background-color:transparent;border:none;color:#666!important;background-position:5px -2px;background-repeat:no-repeat;border-left:1px solid #DDD;display:inline-block;height:21px;line-height:21px;padding:0 5px 0 23px}div.oembedall-githubrepos ul.oembedall-repo-stats li:first-child a{border-left:medium none;margin-right:-3px}div.oembedall-githubrepos ul.oembedall-repo-stats li a:hover{background:5px -27px no-repeat #4183C4;color:#FFF!important;text-decoration:none}div.oembedall-githubrepos ul.oembedall-repo-stats li:first-child a:hover{border-bottom-left-radius:3px;border-top-left-radius:3px}ul.oembedall-repo-stats li:last-child a:hover{border-bottom-right-radius:3px;border-top-right-radius:3px}span.oembedall-closehide{background-color:#aaa;border-radius:2px;cursor:pointer;margin-right:3px}div.oembedall-container{margin-top:5px;text-align:left}.oembedall-ljuser{font-weight:700}.oembedall-ljuser img{vertical-align:bottom;border:0;padding-right:1px}.oembedall-stoqembed{border-bottom:1px dotted #999;float:left;overflow:hidden;width:730px;line-height:1;background:#FFF;color:#000;font-family:Arial,Liberation Sans,DejaVu Sans,sans-serif;font-size:80%;text-align:left;margin:0;padding:0}.oembedall-stoqembed a{color:#07C;text-decoration:none;margin:0;padding:0}.oembedall-stoqembed a:hover{text-decoration:underline}.oembedall-stoqembed a:visited{color:#4A6B82}.oembedall-stoqembed h3{font-family:Trebuchet MS,Liberation Sans,DejaVu Sans,sans-serif;font-size:130%;font-weight:700;margin:0;padding:0}.oembedall-stoqembed .oembedall-reputation-score{color:#444;font-size:120%;font-weight:700;margin-right:2px}.oembedall-stoqembed .oembedall-user-info{height:35px;width:185px}.oembedall-stoqembed .oembedall-user-info .oembedall-user-gravatar32{float:left;height:32px;width:32px}.oembedall-stoqembed .oembedall-user-info .oembedall-user-details{float:left;margin-left:5px;overflow:hidden;white-space:nowrap;width:145px}.oembedall-stoqembed .oembedall-question-hyperlink{font-weight:700}.oembedall-stoqembed .oembedall-stats{background:#EEE;margin:0 0 0 7px;padding:4px 7px 6px;width:58px}.oembedall-stoqembed .oembedall-statscontainer{float:left;margin-right:8px;width:86px}.oembedall-stoqembed .oembedall-votes{color:#555;padding:0 0 7px;text-align:center}.oembedall-stoqembed .oembedall-vote-count-post{font-size:240%;color:#808185;display:block;font-weight:700}.oembedall-stoqembed .oembedall-views{color:#999;padding-top:4px;text-align:center}.oembedall-stoqembed .oembedall-status{margin-top:-3px;padding:4px 0;text-align:center;background:#75845C;color:#FFF}.oembedall-stoqembed .oembedall-status strong{color:#FFF;display:block;font-size:140%}.oembedall-stoqembed .oembedall-summary{float:left;width:635px}.oembedall-stoqembed .oembedall-excerpt{line-height:1.2;margin:0;padding:0 0 5px}.oembedall-stoqembed .oembedall-tags{float:left;line-height:18px}.oembedall-stoqembed .oembedall-tags a:hover{text-decoration:none}.oembedall-stoqembed .oembedall-post-tag{background-color:#E0EAF1;border-bottom:1px solid #3E6D8E;border-right:1px solid #7F9FB6;color:#3E6D8E;font-size:90%;line-height:2.4;margin:2px 2px 2px 0;padding:3px 4px;text-decoration:none;white-space:nowrap}.oembedall-stoqembed .oembedall-post-tag:hover{background-color:#3E6D8E;border-bottom:1px solid #37607D;border-right:1px solid #37607D;color:#E0EAF1}.oembedall-stoqembed .oembedall-fr{float:right}.oembedall-stoqembed .oembedall-statsarrow{background-image:url(http://cdn.sstatic.net/stackoverflow/img/sprites.png?v=3);background-repeat:no-repeat;overflow:hidden;background-position:0 -435px;float:right;height:13px;margin-top:12px;width:7px}.oembedall-facebook1{border:1px solid #1A3C6C;padding:0;font:13.34px/1.4 verdana;width:500px}.oembedall-facebook2{background-color:#627add}.oembedall-facebook2 a{color:#e8e8e8;text-decoration:none}.oembedall-facebookBody{background-color:#fff;vertical-align:top;padding:5px}.oembedall-facebookBody .contents{display:inline-block;width:100%}.oembedall-facebookBody div img{float:left;margin-right:5px}div.oembedall-lanyard{-webkit-box-shadow:none;-webkit-transition-delay:0s;-webkit-transition-duration:.4000000059604645s;-webkit-transition-property:width;-webkit-transition-timing-function:cubic-bezier(0.42,0,.58,1);background-attachment:scroll;background-clip:border-box;background-color:transparent;background-image:none;background-origin:padding-box;border-width:0;box-shadow:none;color:#112644;display:block;float:left;font-family:'Trebuchet MS',Trebuchet,sans-serif;font-size:16px;height:253px;line-height:19px;margin:0;max-width:none;min-height:0;outline:#112644 0;overflow-x:visible;overflow-y:visible;padding:0;position:relative;text-align:left;vertical-align:baseline;width:804px}div.oembedall-lanyard .tagline{font-size:1.5em}div.oembedall-lanyard .wrapper{overflow:hidden;clear:both}div.oembedall-lanyard .split{float:left;display:inline}div.oembedall-lanyard .prominent-place .flag:active,div.oembedall-lanyard .prominent-place .flag:focus,div.oembedall-lanyard .prominent-place .flag:hover,div.oembedall-lanyard .prominent-place .flag:link,div.oembedall-lanyard .prominent-place .flag:visited{float:left;display:block;width:48px;height:48px;position:relative;top:-5px;margin-right:10px}div.oembedall-lanyard .place-context{font-size:.889em}div.oembedall-lanyard .prominent-place .sub-place{display:block}div.oembedall-lanyard .prominent-place{font-size:1.125em;line-height:1.1em;font-weight:400}div.oembedall-lanyard .main-date{color:#8CB4E0;font-weight:700;line-height:1.1}div.oembedall-lanyard .first{width:48.57%;margin:0 0 0 2.857%}.mermaid .label{color:#333}.node circle,.node polygon,.node rect{fill:#cde498;stroke:#13540c;stroke-width:1px}.edgePath .path{stroke:green;stroke-width:1.5px}.cluster rect{fill:#cdffb2;rx:40;stroke:#6eaa49;stroke-width:1px}.cluster text{fill:#333}.actor{stroke:#13540c;fill:#cde498}text.actor{fill:#000;stroke:none}.actor-line{stroke:grey}.messageLine0{stroke-width:1.5;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#333}.messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#arrowhead{fill:#333}#crosshead path{fill:#333!important;stroke:#333!important}.messageText{fill:#333;stroke:none}.labelBox{stroke:#326932;fill:#cde498}.labelText,.loopText{fill:#000;stroke:none}.loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#326932}.note{stroke:#6eaa49;fill:#fff5ad}.noteText{fill:#000;stroke:none;font-family:'trebuchet ms',verdana,arial;font-size:14px}.section{stroke:none;opacity:.2}.section0,.section2{fill:#6eaa49}.section1,.section3{fill:#fff;opacity:.2}.sectionTitle0,.sectionTitle1,.sectionTitle2,.sectionTitle3{fill:#333}.sectionTitle{text-anchor:start;font-size:11px;text-height:14px}.grid .tick{stroke:lightgrey;opacity:.3;shape-rendering:crispEdges}.grid path{stroke-width:0}.today{fill:none;stroke:red;stroke-width:2px}.task{stroke-width:2}.taskText{text-anchor:middle;font-size:11px}.taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}.taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}.taskText0,.taskText1,.taskText2,.taskText3{fill:#fff}.task0,.task1,.task2,.task3{fill:#487e3a;stroke:#13540c}.taskTextOutside0,.taskTextOutside1,.taskTextOutside2,.taskTextOutside3{fill:#000}.active0,.active1,.active2,.active3{fill:#cde498;stroke:#13540c}.activeText0,.activeText1,.activeText2,.activeText3{fill:#000!important}.done0,.done1,.done2,.done3{stroke:grey;fill:lightgrey;stroke-width:2}.doneText0,.doneText1,.doneText2,.doneText3{fill:#000!important}.crit0,.crit1,.crit2,.crit3{stroke:#f88;fill:red;stroke-width:2}.activeCrit0,.activeCrit1,.activeCrit2,.activeCrit3{stroke:#f88;fill:#cde498;stroke-width:2}.doneCrit0,.doneCrit1,.doneCrit2,.doneCrit3{stroke:#f88;fill:lightgrey;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}.activeCritText0,.activeCritText1,.activeCritText2,.activeCritText3,.doneCritText0,.doneCritText1,.doneCritText2,.doneCritText3{fill:#000!important}.titleText{text-anchor:middle;font-size:18px;fill:#000}text{font-family:'trebuchet ms',verdana,arial;font-size:14px}html{height:100%}body{margin:0!important;padding:5px 20px 26px!important;background-color:#fff;font-family:"Lucida Grande","Segoe UI","Apple SD Gothic Neo","Malgun Gothic","Lucida Sans Unicode",Helvetica,Arial,sans-serif;font-size:.9em;overflow-x:hidden;overflow-y:auto}br,h1,h2,h3,h4,h5,h6{clear:both}hr.page{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x;border:0;height:3px;padding:0}hr.underscore{border-top-style:dashed!important}body >:first-child{margin-top:0!important}img.plugin{box-shadow:0 1px 3px rgba(0,0,0,.1);border-radius:3px}iframe{border:0}figure{-webkit-margin-before:0;-webkit-margin-after:0;-webkit-margin-start:0;-webkit-margin-end:0}kbd{border:1px solid #aaa;-moz-border-radius:2px;-webkit-border-radius:2px;border-radius:2px;-moz-box-shadow:1px 2px 2px #ddd;-webkit-box-shadow:1px 2px 2px #ddd;box-shadow:1px 2px 2px #ddd;background-color:#f9f9f9;background-image:-moz-linear-gradient(top,#eee,#f9f9f9,#eee);background-image:-o-linear-gradient(top,#eee,#f9f9f9,#eee);background-image:-webkit-linear-gradient(top,#eee,#f9f9f9,#eee);background-image:linear-gradient(top,#eee,#f9f9f9,#eee);padding:1px 3px;font-family:inherit;font-size:.85em}.oembeded .oembed_photo{display:inline-block}img[data-echo]{margin:25px 0;width:100px;height:100px;background:url(../img/ajax.gif) center center no-repeat #fff}.spinner{display:inline-block;width:10px;height:10px;margin-bottom:-.1em;border:2px solid rgba(0,0,0,.5);border-top-color:transparent;border-radius:100%;-webkit-animation:spin 1s infinite linear;animation:spin 1s infinite linear}.spinner:after{content:'';display:block;width:0;height:0;position:absolute;top:-6px;left:0;border:4px solid transparent;border-bottom-color:rgba(0,0,0,.5);-webkit-transform:rotate(45deg);transform:rotate(45deg)}@-webkit-keyframes spin{to{-webkit-transform:rotate(360deg)}}@keyframes spin{to{transform:rotate(360deg)}}p.toc{margin:0!important}p.toc ul{padding-left:10px}p.toc>ul{padding:10px;margin:0 10px;display:inline-block;border:1px solid #ededed;border-radius:5px}p.toc li,p.toc ul{list-style-type:none}p.toc li{width:100%;padding:0;overflow:hidden}p.toc li a::after{content:"."}p.toc li a:before{content:"• "}p.toc h5{text-transform:uppercase}p.toc .title{float:left;padding-right:3px}p.toc .number{margin:0;float:right;padding-left:3px;background:#fff;display:none}input.task-list-item{margin-left:-1.62em}.markdown{font-family:"Hiragino Sans GB","Microsoft YaHei",STHeiti,SimSun,"Lucida Grande","Lucida Sans Unicode","Lucida Sans",'Segoe UI',AppleSDGothicNeo-Medium,'Malgun Gothic',Verdana,Tahoma,sans-serif;padding:20px}.markdown a{text-decoration:none;vertical-align:baseline}.markdown a:hover{text-decoration:underline}.markdown h1{font-size:2.2em;font-weight:700;margin:1.5em 0 1em}.markdown h2{font-size:1.8em;font-weight:700;margin:1.275em 0 .85em}.markdown h3{font-size:1.6em;font-weight:700;margin:1.125em 0 .75em}.markdown h4{font-size:1.4em;font-weight:700;margin:.99em 0 .66em}.markdown h5{font-size:1.2em;font-weight:700;margin:.855em 0 .57em}.markdown h6{font-size:1em;font-weight:700;margin:.75em 0 .5em}.markdown h1+p,.markdown h1:first-child,.markdown h2+p,.markdown h2:first-child,.markdown h3+p,.markdown h3:first-child,.markdown h4+p,.markdown h4:first-child,.markdown h5+p,.markdown h5:first-child,.markdown h6+p,.markdown h6:first-child{margin-top:0}.markdown hr{border:1px solid #ccc}.markdown p{margin:1em 0;word-wrap:break-word}.markdown ol{list-style-type:decimal}.markdown li{display:list-item;line-height:1.4em}.markdown blockquote{margin:1em 20px}.markdown blockquote>:first-child{margin-top:0}.markdown blockquote>:last-child{margin-bottom:0}.markdown blockquote cite:before{content:'\2014 \00A0'}.markdown .code{border-radius:3px;word-wrap:break-word}.markdown pre{border-radius:3px;word-wrap:break-word;border:1px solid #ccc;overflow:auto;padding:.5em}.markdown pre code{border:0;display:block}.markdown pre>code{font-family:Consolas,Inconsolata,Courier,monospace;font-weight:700;white-space:pre;margin:0}.markdown code{border-radius:3px;word-wrap:break-word;border:1px solid #ccc;padding:0 5px;margin:0 2px}.markdown img{max-width:100%}.markdown mark{color:#000;background-color:#fcf8e3}.markdown table{padding:0;border-collapse:collapse;border-spacing:0;margin-bottom:16px}.markdown table tr td,.markdown table tr th{border:1px solid #ccc;margin:0;padding:6px 13px}.markdown table tr th{font-weight:700}.markdown table tr th>:first-child{margin-top:0}.markdown table tr th>:last-child{margin-bottom:0}.markdown table tr td>:first-child{margin-top:0}.markdown table tr td>:last-child{margin-bottom:0}@import url(http://fonts.googleapis.com/css?family=Roboto+Condensed:300italic,400italic,700italic,400,300,700);.haroopad{padding:20px;color:#222;font-size:15px;font-family:"Roboto Condensed",Tauri,"Hiragino Sans GB","Microsoft YaHei",STHeiti,SimSun,"Lucida Grande","Lucida Sans Unicode","Lucida Sans",'Segoe UI',AppleSDGothicNeo-Medium,'Malgun Gothic',Verdana,Tahoma,sans-serif;background:#fff;line-height:1.6;-webkit-font-smoothing:antialiased}.haroopad a{color:#3269a0}.haroopad a:hover{color:#4183c4}.haroopad h2{border-bottom:1px solid #e6e6e6}.haroopad h6{color:#777}.haroopad hr{border:1px solid #e6e6e6}.haroopad blockquote>code,.haroopad h1>code,.haroopad h2>code,.haroopad h3>code,.haroopad h4>code,.haroopad h5>code,.haroopad h6>code,.haroopad li>code,.haroopad p>code,.haroopad td>code{font-family:Consolas,"Liberation Mono",Menlo,Courier,monospace;font-size:85%;background-color:rgba(0,0,0,.02);padding:.2em .5em;border:1px solid #efefef}.haroopad pre>code{font-size:1em;letter-spacing:-1px;font-weight:700}.haroopad blockquote{border-left:4px solid #e6e6e6;padding:0 15px;color:#777}.haroopad table{background-color:#fafafa}.haroopad table tr td,.haroopad table tr th{border:1px solid #e6e6e6}.haroopad table tr:nth-child(2n){background-color:#f2f2f2}.hljs{display:block;overflow-x:auto;padding:.5em;background:#fdf6e3;color:#657b83;-webkit-text-size-adjust:none}.diff .hljs-header,.hljs-comment,.hljs-doctype,.hljs-javadoc,.hljs-pi,.lisp .hljs-string{color:#93a1a1}.css .hljs-tag,.hljs-addition,.hljs-keyword,.hljs-request,.hljs-status,.hljs-winutils,.method,.nginx .hljs-title{color:#859900}.hljs-command,.hljs-dartdoc,.hljs-hexcolor,.hljs-link_url,.hljs-number,.hljs-phpdoc,.hljs-regexp,.hljs-rules .hljs-value,.hljs-string,.hljs-tag .hljs-value,.tex .hljs-formula{color:#2aa198}.css .hljs-function,.hljs-built_in,.hljs-chunk,.hljs-decorator,.hljs-id,.hljs-identifier,.hljs-localvars,.hljs-title,.vhdl .hljs-literal{color:#268bd2}.hljs-attribute,.hljs-class .hljs-title,.hljs-constant,.hljs-link_reference,.hljs-parent,.hljs-type,.hljs-variable,.lisp .hljs-body,.smalltalk .hljs-number{color:#b58900}.css .hljs-pseudo,.diff .hljs-change,.hljs-attr_selector,.hljs-cdata,.hljs-header,.hljs-pragma,.hljs-preprocessor,.hljs-preprocessor .hljs-keyword,.hljs-shebang,.hljs-special,.hljs-subst,.hljs-symbol,.hljs-symbol .hljs-string{color:#cb4b16}.hljs-deletion,.hljs-important{color:#dc322f}.hljs-link_label{color:#6c71c4}.tex .hljs-formula{background:#eee8d5}.MathJax_Hover_Frame{border-radius:.25em;-webkit-border-radius:.25em;-moz-border-radius:.25em;-khtml-border-radius:.25em;box-shadow:0 0 15px #83A;-webkit-box-shadow:0 0 15px #83A;-moz-box-shadow:0 0 15px #83A;-khtml-box-shadow:0 0 15px #83A;border:1px solid #A6D!important;display:inline-block;position:absolute}.MathJax_Hover_Arrow{position:absolute;width:15px;height:11px;cursor:pointer}#MathJax_About{position:fixed;left:50%;width:auto;text-align:center;border:3px outset;padding:1em 2em;background-color:#DDD;color:#000;cursor:default;font-family:message-box;font-size:120%;font-style:normal;text-indent:0;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal;word-wrap:normal;white-space:nowrap;float:none;z-index:201;border-radius:15px;-webkit-border-radius:15px;-moz-border-radius:15px;-khtml-border-radius:15px;box-shadow:0 10px 20px gray;-webkit-box-shadow:0 10px 20px gray;-moz-box-shadow:0 10px 20px gray;-khtml-box-shadow:0 10px 20px gray;filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}.MathJax_Menu{position:absolute;background-color:#fff;color:#000;width:auto;padding:5px 0;border:1px solid #CCC;margin:0;cursor:default;font:menu;text-align:left;text-indent:0;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal;word-wrap:normal;white-space:nowrap;float:none;z-index:201;border-radius:5px;-webkit-border-radius:5px;-moz-border-radius:5px;-khtml-border-radius:5px;box-shadow:0 10px 20px gray;-webkit-box-shadow:0 10px 20px gray;-moz-box-shadow:0 10px 20px gray;-khtml-box-shadow:0 10px 20px gray;filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}.MathJax_MenuItem{padding:1px 2em;background:0 0}.MathJax_MenuArrow{position:absolute;right:.5em;color:#666}.MathJax_MenuActive .MathJax_MenuArrow{color:#fff}.MathJax_MenuArrow.RTL{left:.5em;right:auto}.MathJax_MenuCheck{position:absolute;left:.7em}.MathJax_MenuCheck.RTL{right:.7em;left:auto}.MathJax_MenuRadioCheck{position:absolute;left:.7em}.MathJax_MenuRadioCheck.RTL{right:.7em;left:auto}.MathJax_MenuLabel{padding:1px 2em 3px 1.33em;font-style:italic}.MathJax_MenuRule{border-top:1px solid #DDD;margin:4px 3px}.MathJax_MenuDisabled{color:GrayText}.MathJax_MenuActive{background-color:#606872;color:#fff}.MathJax_Menu_Close{position:absolute;width:31px;height:31px;top:-15px;left:-15px}#MathJax_Zoom{position:absolute;background-color:#F0F0F0;overflow:auto;display:block;z-index:301;padding:.5em;border:1px solid #000;margin:0;font-weight:400;font-style:normal;text-align:left;text-indent:0;text-transform:none;line-height:normal;letter-spacing:normal;word-spacing:normal;word-wrap:normal;white-space:nowrap;float:none;box-shadow:5px 5px 15px #AAA;-webkit-box-shadow:5px 5px 15px #AAA;-moz-box-shadow:5px 5px 15px #AAA;-khtml-box-shadow:5px 5px 15px #AAA;filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}#MathJax_ZoomOverlay{position:absolute;left:0;top:0;z-index:300;display:inline-block;width:100%;height:100%;border:0;padding:0;margin:0;background-color:#fff;opacity:0;filter:alpha(opacity=0)}#MathJax_ZoomFrame{position:relative;display:inline-block;height:0;width:0}#MathJax_ZoomEventTrap{position:absolute;left:0;top:0;z-index:302;display:inline-block;border:0;padding:0;margin:0;background-color:#fff;opacity:0;filter:alpha(opacity=0)}.MathJax_Preview{color:#888}#MathJax_Message{position:fixed;left:1px;bottom:2px;background-color:#E6E6E6;border:1px solid #959595;margin:0;padding:2px 8px;z-index:102;color:#000;font-size:80%;width:auto;white-space:nowrap}#MathJax_MSIE_Frame{position:absolute;top:0;left:0;width:0;z-index:101;border:0;margin:0;padding:0}.MathJax_Error{color:#C00;font-style:italic}footer{position:fixed;font-size:.8em;text-align:right;bottom:0;margin-left:-25px;height:20px;width:100%}</style>
</head>
<body class="markdown haroopad">
<h1 id="wtf-deep-learning!!!"><a name="wtf-deep-learning!!!" href="#wtf-deep-learning!!!"></a>WTF Deep Learning!!!</h1><h2 id="table-of-content"><a name="table-of-content" href="#table-of-content"></a>Table Of Content</h2><ul>
<li><a href="#github">Github</a></li><li><a href="#paper">Paper</a><ul>
<li><a href="#survey-review">Survey Review</a></li><li><a href="#theory-future">Theory Future</a></li><li><a href="#optimization-regularization">Optimization Regularization</a></li><li><a href="#networkmodels">NetworkModels</a></li><li><a href="#image">Image</a></li><li><a href="#caption">Caption</a></li><li><a href="video-human-activity">Video Human Activity</a></li><li><a href="#word-embedding">Word Embedding</a></li><li><a href="#machine-translation-qna">Machine Translation QnA</a></li><li><a href="#speech-etc">Speech Etc</a></li><li><a href="#rl-robotics">RL Robotics</a></li><li><a href="#unsupervised">Unsupervised</a></li><li><a href="#hardware-software">Hardware Software</a></li><li><a href="#bayesian">Bayesian</a></li></ul>
</li><li><a href="#license">License</a></li></ul><h2 id="github"><a name="github" href="#github"></a>Github</h2><pre class="bash hljs"><code class="bash" data-origin="<pre><code class=&quot;bash&quot;>git clone https://github.com/wtf-deeplearning/wtf-deeplearning.github.io.git
</code></pre>">git <span class="hljs-built_in">clone</span> https://github.com/wtf-deeplearning/wtf-deeplearning.github.io.git
</code></pre><h2 id="paper"><a name="paper" href="#paper"></a>Paper</h2><h3 id="survey-review"><a name="survey-review" href="#survey-review"></a>Survey Review</h3><ul>
<li>Deep learning (2015), Y. LeCun, Y. Bengio and G. Hinton <a href="survey-review/NatureDeepReview.pdf">[pdf]</a><br>source: <a href="https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf">https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf</a></li><li>Deep learning in neural networks: An overview (2015), J. Schmidhuber <a href="survey-review/DeepLearningInNeuralNetworksOverview.JSchmidhuber2015.pdf">[pdf]</a><br>source: <a href="http://www2.econ.iastate.edu/tesfatsi/DeepLearningInNeuralNetworksOverview.JSchmidhuber2015.pdf">http://www2.econ.iastate.edu/tesfatsi/DeepLearningInNeuralNetworksOverview.JSchmidhuber2015.pdf</a></li><li>Representation learning: A review and new perspectives (2013), Y. Bengio et al. <a href="survey-review/BengioETAL12.pdf">[pdf]</a><br>source : <a href="http://www.cl.uni-heidelberg.de/courses/ws14/deepl/BengioETAL12.pdf">http://www.cl.uni-heidelberg.de/courses/ws14/deepl/BengioETAL12.pdf</a></li></ul><h3 id="theory-future"><a name="theory-future" href="#theory-future"></a>Theory Future</h3><ul>
<li>Distilling the knowledge in a neural network (2015), G. Hinton et al. <a href="theory-future/1503.02531.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1503.02531">http://arxiv.org/pdf/1503.02531</a></li><li>Deep neural networks are easily fooled: High confidence predictions for unrecognizable images (2015), A. Nguyen et al. <a href="theory-future/1412.1897.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1412.1897">http://arxiv.org/pdf/1412.1897</a></li><li>How transferable are features in deep neural networks? (2014), J. Yosinski et al. <em>(Bengio)</em> <a href="theory-future/5347-how-transferable-are-features-in-deep-neural-networks.pdf">[pdf]</a><br>source : <a href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf">http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf</a></li><li>Why does unsupervised pre-training help deep learning (2010), E. Erhan et al. <em>(Bengio)</em> <a href="theory-future/AISTATS2010_ErhanCBV10.pdf">[pdf]</a><br>source : <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf">http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf</a></li><li>Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio <a href="theory-future/AISTATS2010_GlorotB10.pdf">[pdf]</a><br>source : <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf">http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf</a></li></ul><h3 id="optimization-regularization"><a name="optimization-regularization" href="#optimization-regularization"></a>Optimization Regularization</h3><ul>
<li>Taking the human out of the loop: A review of bayesian optimization (2016), B. Shahriari et al. <a href="optimization-regularization/BayesOptLoop.pdf">[pdf]</a><br>source : <a href="https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf">https://www.cs.ox.ac.uk/people/nando.defreitas/publications/BayesOptLoop.pdf</a></li><li>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (2015), S. Loffe and C. Szegedy <a href="optimization-regularization/1502.03167.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1502.03167">http://arxiv.org/pdf/1502.03167</a></li><li>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015), K. He et al. <a href="optimization-regularization/He_Delving_Deep_into_ICCV_2015_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf</a></li><li>Dropout: A simple way to prevent neural networks from overfitting (2014), N. Srivastava et al. <em>(Hinton)</em> <a href="optimization-regularization/srivastava14a.pdf">[pdf]</a><br>source : <a href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf</a></li><li>Adam: A method for stochastic optimization (2014), D. Kingma and J. Ba <a href="optimization-regularization/1412.6980.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1412.6980">http://arxiv.org/pdf/1412.6980</a></li><li>Regularization of neural networks using dropconnect (2013), L. Wan et al. <em>(LeCun)</em> <a href="optimization-regularization/icml2013_wan13.pdf">[pdf]</a><br>source : <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf">http://machinelearning.wustl.edu/mlpapers/paper_files/icml2013_wan13.pdf</a></li><li>Improving neural networks by preventing co-adaptation of feature detectors (2012), G. Hinton et al. <a href="optimization-regularization/1207.0580.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1207.0580.pdf">http://arxiv.org/pdf/1207.0580.pdf</a></li><li>Spatial pyramid pooling in deep convolutional networks for visual recognition (2014), K. He et al. <a href="optimization-regularization/1406.4729">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1406.4729">http://arxiv.org/pdf/1406.4729</a></li><li>Random search for hyper-parameter optimization (2012) J. Bergstra and Y. Bengio <a href="optimization-regularization/bergstra12a.pdf">[pdf]</a><br>source : <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a">http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a</a></li></ul><h3 id="networkmodels"><a name="networkmodels" href="#networkmodels"></a>NetworkModels</h3><ul>
<li>Deep residual learning for image recognition (2016), K. He et al. <em>(Microsoft)</em> <a href="networkmodels/1512.03385">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1512.03385">http://arxiv.org/pdf/1512.03385</a></li><li>Going deeper with convolutions (2015), C. Szegedy et al. <em>(Google)</em> <a href="networkmodels/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf</a></li><li>Fast R-CNN (2015), R. Girshick <a href="networkmodels/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf</a></li><li>Very deep convolutional networks for large-scale image recognition (2014), K. Simonyan and A. Zisserman <a href="networkmodels/1409.1556.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1409.1556">http://arxiv.org/pdf/1409.1556</a></li><li>Fully convolutional networks for semantic segmentation (2015), J. Long et al. <a href="networkmodels/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf</a></li><li>OverFeat: Integrated recognition, localization and detection using convolutional networks (2014), P. Sermanet et al. <em>(LeCun)</em> <a href="networkmodels/1312.6229.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1312.6229">http://arxiv.org/pdf/1312.6229</a></li><li>Visualizing and understanding convolutional networks (2014), M. Zeiler and R. Fergus <a href="networkmodels/1311.2901.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1311.2901">http://arxiv.org/pdf/1311.2901</a></li><li>Maxout networks (2013), I. Goodfellow et al. <em>(Bengio)</em> <a href="networkmodels/1302.4389v4.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1302.4389v4">http://arxiv.org/pdf/1302.4389v4</a></li><li>ImageNet classification with deep convolutional neural networks (2012), A. Krizhevsky et al. <em>(Hinton)</em> <a href="networkmodels/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">[pdf]</a><br>source : <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></li><li>Large scale distributed deep networks (2012), J. Dean et al. <a href="networkmodels/4687-large-scale-distributed-deep-networks.pdf">[pdf]</a><br>source : <a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf">http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf</a></li><li>Deep sparse rectifier neural networks (2011), X. Glorot et al. <em>(Bengio)</em> <a href="networkmodels/AISTATS2011_GlorotBB11.pdf">[pdf]</a><br>source : <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf">http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf</a></li></ul><h3 id="image"><a name="image" href="#image"></a>Image</h3><ul>
<li>Imagenet large scale visual recognition challenge (2015), O. Russakovsky et al. <a href="image/1409.0575">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1409.0575">http://arxiv.org/pdf/1409.0575</a></li><li>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2015), S. Ren et al. <a href="image/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">[pdf]</a><br>source : <a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf</a></li><li>DRAW: A recurrent neural network for image generation (2015), K. Gregor et al. <a href="image/1502.04623.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1502.04623">http://arxiv.org/pdf/1502.04623</a></li><li>Rich feature hierarchies for accurate object detection and semantic segmentation (2014), R. Girshick et al. <a href="image/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf</a></li><li>Learning and transferring mid-Level image representations using convolutional neural networks (2014), M. Oquab et al. <a href="image/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf</a></li><li>DeepFace: Closing the Gap to Human-Level Performance in Face Verification (2014), Y. Taigman et al. <em>(Facebook)</em> <a href="image/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf</a></li><li>Decaf: A deep convolutional activation feature for generic visual recognition (2013), J. Donahue et al. <a href="image/1310.1531.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1310.1531">http://arxiv.org/pdf/1310.1531</a></li><li>Learning Hierarchical Features for Scene Labeling (2013), C. Farabet et al. <em>(LeCun)</em> <a href="image/farabet-pami-13.pdf">[pdf]</a><br>source : <a href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf">https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/PDF/farabet-pami-13.pdf</a></li><li>Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis (2011), Q. Le et al. <a href="image/cvpr_LeZouYeungNg11.pdf">[pdf]</a><br>source : <a href="http://robotics.stanford.edu/~wzou/cvpr_LeZouYeungNg11.pdf">http://robotics.stanford.edu/~wzou/cvpr_LeZouYeungNg11.pdf</a></li><li>Learning mid-level features for recognition (2010), Y. Boureau <em>(LeCun)</em> <a href="image/boureau-cvpr-10.pdf">[pdf]</a><br>source : <a href="http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf">http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf</a></li></ul><h3 id="caption"><a name="caption" href="#caption"></a>Caption</h3><ul>
<li>Show, attend and tell: Neural image caption generation with visual attention (2015), K. Xu et al. <em>(Bengio)</em> <a href="caption/1502.03044.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1502.03044">http://arxiv.org/pdf/1502.03044</a></li><li>Show and tell: A neural image caption generator (2015), O. Vinyals et al. <a href="caption/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf</a></li><li>Long-term recurrent convolutional networks for visual recognition and description (2015), J. Donahue et al. <a href="caption/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf</a></li><li>Deep visual-semantic alignments for generating image descriptions (2015), A. Karpathy and L. Fei-Fei <a href="caption/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf</a></li></ul><h3 id="video-human-activity"><a name="video-human-activity" href="#video-human-activity"></a>Video Human Activity</h3><ul>
<li>Large-scale video classification with convolutional neural networks (2014), A. Karpathy et al. <em>(FeiFei)</em> <a href="video-human-activity/karpathy14.pdf">[pdf]</a><br>source : vision.stanford.edu/pdf/karpathy14.pdf</li><li>A survey on human activity recognition using wearable sensors (2013), O. Lara and M. Labrador <a href="video-human-activity/Lara%20-%20Human%20Activity%20Recognition%20-%202013.pdf">[pdf]</a><br>source : <a href="http://romisatriawahono.net/lecture/rm/survey/computer%20vision/Lara%20-%20Human%20Activity%20Recognition%20-%202013.pdf">http://romisatriawahono.net/lecture/rm/survey/computer%20vision/Lara%20-%20Human%20Activity%20Recognition%20-%202013.pdf</a></li><li>3D convolutional neural networks for human action recognition (2013), S. Ji et al. <a href="video-human-activity/icml2010_JiXYY10.pdf">[pdf]</a><br>source : <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf">http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf</a></li><li>Deeppose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy <a href="video-human-activity/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf</a></li><li>Action recognition with improved trajectories (2013), H. Wang and C. Schmid <a href="video-human-activity/Wang_Action_Recognition_with_2013_ICCV_paper.pdf">[pdf]</a><br>source : <a href="http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Action_Recognition_with_2013_ICCV_paper.pdf">http://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Wang_Action_Recognition_with_2013_ICCV_paper.pdf</a></li></ul><h3 id="word-embedding"><a name="word-embedding" href="#word-embedding"></a>Word Embedding</h3><ul>
<li>Glove: Global vectors for word representation (2014), J. Pennington et al. <a href="word-embedding/nn-pres.pdf">[pdf]</a><br>source : <a href="http://llcao.net/cu-deeplearning15/presentation/nn-pres.pdf">http://llcao.net/cu-deeplearning15/presentation/nn-pres.pdf</a></li><li>Sequence to sequence learning with neural networks (2014), I. Sutskever et al. <a href="word-embedding/5346-sequence-to-sequence-learning-with-neural-networks.pdf">[pdf]</a><br>source : <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf</a></li><li>Distributed representations of sentences and documents (2014), Q. Le and T. Mikolov <a href="word-embedding/1405.4053.pdf">[pdf]</a> <em>(Google)</em><br>source : <a href="http://arxiv.org/pdf/1405.4053">http://arxiv.org/pdf/1405.4053</a></li><li>Distributed representations of words and phrases and their compositionality (2013), T. Mikolov et al. <em>(Google)</em> <a href="word-embedding/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">[pdf]</a><br>source : <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a></li><li>Efficient estimation of word representations in vector space (2013), T. Mikolov et al. <em>(Google)</em> <a href="word-embedding/1301.3781.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1301.3781">http://arxiv.org/pdf/1301.3781</a></li><li>Word representations: a simple and general method for semi-supervised learning (2010), J. Turian <em>(Bengio)</em> <a href="word-embedding/P10-1040.pdf">[pdf]</a><br>source : <a href="http://www.anthology.aclweb.org/P/P10/P10-1040.pdf">http://www.anthology.aclweb.org/P/P10/P10-1040.pdf</a></li></ul><h3 id="machine-translation-qna"><a name="machine-translation-qna" href="#machine-translation-qna"></a>Machine Translation QnA</h3><ul>
<li>Towards ai-complete question answering: A set of prerequisite toy tasks (2015), J. Weston et al. <a href="machine-translation/1502.05698.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1502.05698">http://arxiv.org/pdf/1502.05698</a></li><li>Neural machine translation by jointly learning to align and translate (2014), D. Bahdanau et al. <em>(Bengio)</em> <a href="machine-translation/1409.0473.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1409.0473">http://arxiv.org/pdf/1409.0473</a></li><li>Learning phrase representations using RNN encoder-decoder for statistical machine translation (2014), K. Cho et al. <em>(Bengio)</em> <a href="machine-translation/1406.1078.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1406.1078">http://arxiv.org/pdf/1406.1078</a></li><li>A convolutional neural network for modelling sentences (2014), N. kalchbrenner et al. <a href="machine-translation/1404.2188v1.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1404.2188v1">http://arxiv.org/pdf/1404.2188v1</a></li><li>Convolutional neural networks for sentence classification (2014), Y. Kim <a href="machine-translation/1408.5882.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1408.5882">http://arxiv.org/pdf/1408.5882</a></li><li>The stanford coreNLP natural language processing toolkit (2014), C. Manning et al. <a href="machine-translation/acl2014-corenlp.pdf">[pdf]</a><br>source : <a href="http://www.surdeanu.info/mihai/papers/acl2014-corenlp.pdf">http://www.surdeanu.info/mihai/papers/acl2014-corenlp.pdf</a></li><li>Recursive deep models for semantic compositionality over a sentiment treebank (2013), R. Socher et al. <a href="machine-translation/10.1.1.383.1327.pdf">[pdf]</a><br>source : <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;rep=rep1&amp;type=pdf</a></li><li>Natural language processing (almost) from scratch (2011), R. Collobert et al. <a href="machine-translation/1103.0398.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1103.0398">http://arxiv.org/pdf/1103.0398</a></li><li>Recurrent neural network based language model (2010), T. Mikolov et al. <a href="machine-translation/rnnlm_mikolov.pdf">[pdf]</a><br>source : <a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf">http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf</a></li></ul><h3 id="speech-etc"><a name="speech-etc" href="#speech-etc"></a>Speech Etc</h3><ul>
<li>Speech recognition with deep recurrent neural networks (2013), A. Graves <em>(Hinton)</em> <a href="speech/1303.5778.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1303.5778.pdf">http://arxiv.org/pdf/1303.5778.pdf</a></li><li>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups (2012), G. Hinton et al. <a href="speech/SPM_DNN_12.pdf">[pdf]</a><br>source : <a href="http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf">http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf</a></li><li>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition (2012) G. Dahl et al. <a href="speech/10.1.1.337.7548.pdf">[pdf]</a><br>source : <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;rep=rep1&amp;type=pdf</a></li></ul><h2 id="rl-robotics"><a name="rl-robotics" href="#rl-robotics"></a>RL Robotics</h2><ul>
<li>Mastering the game of Go with deep neural networks and tree search, D. Silver et al. <em>(DeepMind)</em> <a href="robotics/AlphaGoNaturePaper.pdf">[pdf]</a><br>source : <a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf">https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf</a></li><li>Human-level control through deep reinforcement learning (2015), V. Mnih et al. <em>(DeepMind)</em> <a href="robotics/nature14236.pdf">[pdf]</a><br>source : <a href="http://www.davidqiu.com:8888/research/nature14236.pdf">http://www.davidqiu.com:8888/research/nature14236.pdf</a></li><li>Deep learning for detecting robotic grasps (2015), I. Lenz et al. <a href="robotics/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf">[pdf]</a><br>source : <a href="http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf">http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf</a></li><li>Playing atari with deep reinforcement learning (2013), V. Mnih et al. <em>(DeepMind)</em> <a href="robotics/1312.5602.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1312.5602.pdf">http://arxiv.org/pdf/1312.5602.pdf</a></li></ul><h3 id="unsupervised"><a name="unsupervised" href="#unsupervised"></a>Unsupervised</h3><ul>
<li>Building high-level features using large scale unsupervised learning (2013), Q. Le et al. <a href="unsupervised/1112.6209.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1112.6209">http://arxiv.org/pdf/1112.6209</a></li><li>Contractive auto-encoders: Explicit invariance during feature extraction (2011), S. Rifai et al. <em>(Bengio)</em> <a href="unsupervised/ICML2011Rifai_455.pdf">[pdf]</a><br>source : <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf">http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf</a></li><li>An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. <a href="unsupervised/AISTATS2011_CoatesNL11.pdf">[pdf]</a><br>source : <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf">http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf</a></li><li>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. <em>(Bengio)</em> <a href="unsupervised/vincent10a.pdf">[pdf]</a><br>source : <a href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf">http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf</a></li><li>A practical guide to training restricted boltzmann machines (2010), G. Hinton <a href="unsupervised/guideTR.pdf">[pdf]</a><br>source : <a href="http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf">http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf</a></li></ul><h3 id="hardware-software"><a name="hardware-software" href="#hardware-software"></a>Hardware Software</h3><ul>
<li>TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems (2016), M. Abadi et al. <em>(Google)</em> <a href="hardware-software/1603.04467.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1603.04467">http://arxiv.org/pdf/1603.04467</a></li><li>MatConvNet: Convolutional neural networks for matlab (2015), A. Vedaldi and K. Lenc <a href="hardware-software/1412.4564.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1412.4564">http://arxiv.org/pdf/1412.4564</a></li><li>Caffe: Convolutional architecture for fast feature embedding (2014), Y. Jia et al. <a href="hardware-software/1408.5093.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1408.5093">http://arxiv.org/pdf/1408.5093</a></li><li>Theano: new features and speed improvements (2012), F. Bastien et al. <em>(Bengio)</em> <a href="hardware-software/1211.5590.pdf">[pdf]</a><br>source : <a href="http://arxiv.org/pdf/1211.5590">http://arxiv.org/pdf/1211.5590</a></li></ul><h3 id="bayesian"><a name="bayesian" href="#bayesian"></a>Bayesian</h3><h4 id="2013:"><a name="2013:" href="#2013:"></a>2013:</h4><ol>
<li>Deep gaussian processes|Andreas C. Damianou,Neil D. Lawrence|2013 <br><br>Source: <a href="http://www.jmlr.org/proceedings/papers/v31/damianou13a.pdf">http://www.jmlr.org/proceedings/papers/v31/damianou13a.pdf</a></li></ol><h4 id="2014:"><a name="2014:" href="#2014:"></a>2014:</h4><ol>
<li>Avoiding pathologies in very deep networks|D Duvenaud, O Rippel, R Adams|2014 <br><br>Source: <a href="http://www.jmlr.org/proceedings/papers/v33/duvenaud14.pdf">http://www.jmlr.org/proceedings/papers/v33/duvenaud14.pdf</a></li><li>Nested variational compression in deep Gaussian processes|J Hensman, ND Lawrence|2014<br>Source: <a href="https://arxiv.org/abs/1412.1370">https://arxiv.org/abs/1412.1370</a></li></ol><h4 id="2015:"><a name="2015:" href="#2015:"></a>2015:</h4><ol>
<li>On Modern Deep Learning and Variational Inference  |Yarin Gal, Zoubin Ghahramani|2015 <br><br>Source: <a href="http://www.approximateinference.org/accepted/GalGhahramani2015.pdf">http://www.approximateinference.org/accepted/GalGhahramani2015.pdf</a></li><li>Rapid Prototyping of Probabilistic Models: Emerging Challenges in Variational Inference   |Yarin Gal, |2015<br><br>Source: <a href="http://www.approximateinference.org/accepted/Gal2015.pdf">http://www.approximateinference.org/accepted/Gal2015.pdf</a> </li><li>Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference |Yarin Gal, Zoubin Ghahramani|2015<br><br>Source: <a href="http://arxiv.org/abs/1506.02158">http://arxiv.org/abs/1506.02158</a></li><li>Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning  |Yarin Gal, Zoubin Ghahramani|2015<br><br>Source: <a href="http://arxiv.org/abs/1506.02142">http://arxiv.org/abs/1506.02142</a></li><li>Dropout as a Bayesian Approximation: Insights and Applications     |Yarin Gal, |2015<br>Source: <a href="https://sites.google.com/site/deeplearning2015/33.pdf?attredirects=0">https://sites.google.com/site/deeplearning2015/33.pdf?attredirects=0</a></li><li>Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference |Yarin Gal, Zoubin Ghahramani|2015<br><br>Source: <a href="http://arxiv.org/abs/1506.02158">http://arxiv.org/abs/1506.02158</a></li><li>Scalable Variational Gaussian Process Classification|J Hensman, AGG Matthews, Z Ghahramani|2015<br>Source: <a href="http://www.jmlr.org/proceedings/papers/v38/hensman15.pdf">http://www.jmlr.org/proceedings/papers/v38/hensman15.pdf</a></li></ol><h4 id="2016:"><a name="2016:" href="#2016:"></a>2016:</h4><ol>
<li>Relativistic Monte Carlo | Xiaoyu Lu| 2016 <br><br>Source: <a href="https://arxiv.org/abs/1609.04388">https://arxiv.org/abs/1609.04388</a></li><li>Risk versus Uncertainty in Deep Learning: Bayes, Bootstrap and the Dangers of Dropout | Ian Osband| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_4.pdf">http://bayesiandeeplearning.org/papers/BDL_4.pdf</a></li><li>Semi-supervised deep kernel learning|Neal Jean, Michael Xie, Stefano Ermon|2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_5.pdf">http://bayesiandeeplearning.org/papers/BDL_5.pdf</a></li><li>Categorical Reparameterization with Gumbel-Softmax| Eric Jang, Shixiang Gu,Ben Poole| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_8.pdf">http://bayesiandeeplearning.org/papers/BDL_8.pdf</a><br>Video: <a href="https://www.youtube.com/watch?v=JFgXEbgcT7g">https://www.youtube.com/watch?v=JFgXEbgcT7g</a></li><li>Learning to Optimise: Using Bayesian Deep Learning for Transfer Learning in Optimisation| Jonas Langhabel,   Jannik Wolff| 2016<br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_9.pdf">http://bayesiandeeplearning.org/papers/BDL_9.pdf</a></li><li>One-Shot Learning in Discriminative Neural Networks| Jordan Burgess,James Robert Lloyd,Zoubin Ghahramani| 2016<br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_10.pdf">http://bayesiandeeplearning.org/papers/BDL_10.pdf</a></li><li>Distributed Bayesian Learning with Stochastic Natural-gradient Expectation Propagation| Leonard Hasenclever,<br>Stefan Webb| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_11.pdf">http://bayesiandeeplearning.org/papers/BDL_11.pdf</a></li><li>Knots in random neural networks| Kevin K. Chen| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_2.pdf">http://bayesiandeeplearning.org/papers/BDL_2.pdf</a></li><li>Discriminative Bayesian neural networks know what they do not know | Christian Leibig, Siegfried Wahl| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_12.pdf">http://bayesiandeeplearning.org/papers/BDL_12.pdf</a></li><li>Variational Inference in Neural Networks using an Approximate Closed-Form Objective|Wolfgang Roth and Franz Pernkopf|2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_13.pdf">http://bayesiandeeplearning.org/papers/BDL_13.pdf</a></li><li>Combining sequential deep learning and variational Bayes for semi-supervised inference| Jos van der Westhuizen, Dr. Joan Lasenby| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_14.pdf">http://bayesiandeeplearning.org/papers/BDL_14.pdf</a></li><li>Importance Weighted Autoencoders with Random Neural Network Parameters| Daniel Hernández-Lobato,Thang D. Bui,Yinzhen Li| 2016<br>Stefan Webb| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_15.pdf">http://bayesiandeeplearning.org/papers/BDL_15.pdf</a></li><li>Variational Graph Auto-Encoders| Thomas N. Kipf,Max Welling| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_16.pdf">http://bayesiandeeplearning.org/papers/BDL_16.pdf</a></li><li>Dropout-based Automatic Relevance Determination| Dmitry Molchanov, Arseniy Ashuha, Dmitry Vetrov| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_18.pdf">http://bayesiandeeplearning.org/papers/BDL_18.pdf</a></li><li>Scalable GP-LSTMs with Semi-Stochastic Gradients| Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu and Eric P. Xing| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_19.pdf">http://bayesiandeeplearning.org/papers/BDL_19.pdf</a></li><li>Approximate Inference for Deep Latent Gaussian Mixture Models|Eric Nalisnick, Lars Hertel and Padhraic Smyth|2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_20.pdf">http://bayesiandeeplearning.org/papers/BDL_20.pdf</a></li><li>Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Training | Dilin Wang, Yihao Feng and Qiang Liu| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_21.pdf">http://bayesiandeeplearning.org/papers/BDL_21.pdf</a><br>Video: <a href="https://www.youtube.com/watch?v=fi-UUQe2Pss">https://www.youtube.com/watch?v=fi-UUQe2Pss</a></li><li>Learning and Policy Search in Stochastic Dynamical Systems with Bayesian Neural Networks| Stefan Depeweg, José Miguel Hernández-Lobato, Finale Doshi-Velez and Steffen Udluft| 2016<br><br>Source: <a href="https://arxiv.org/abs/1605.07127">https://arxiv.org/abs/1605.07127</a></li><li>Accelerating Deep Gaussian Processes Inference with Arc-Cosine Kernels  | Kurt Cutajar, Edwin V. Bonilla, Pietro Michiardi and Maurizio Filippone| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_24.pdf">http://bayesiandeeplearning.org/papers/BDL_24.pdf</a></li><li>Embedding Words as Distributions with a Bayesian Skip-gram Model | Arthur Bražinskas, Serhii Havrylov and Ivan Titov| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_25.pdf">http://bayesiandeeplearning.org/papers/BDL_25.pdf</a></li><li>Variational Inference on Deep Exponential Family by using Variational Inferences on Conjugate Models|Mohammad Emtiyaz Khan and Wu Lin|2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_26.pdf">http://bayesiandeeplearning.org/papers/BDL_26.pdf</a></li><li>Neural Variational Inference for Latent Dirichlet Allocation| Akash Srivastava and Charles Sutton| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_27.pdf">http://bayesiandeeplearning.org/papers/BDL_27.pdf</a></li><li>Hierarchical Bayesian Neural Networks for Personalized Classification | Ajjen Joshi, Soumya Ghosh, Margrit Betke and Hanspeter Pfister| 2016<br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_28.pdf">http://bayesiandeeplearning.org/papers/BDL_28.pdf</a></li><li>Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles| Balaji Lakshminarayanan, Alexander Pritzel and Charles Blundell| 2016<br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_29.pdf">http://bayesiandeeplearning.org/papers/BDL_29.pdf</a></li><li>Asynchronous Stochastic Gradient MCMC with Elastic Coupling| Jost Tobias Springenberg, Aaron Klein, Stefan Falkner and Frank Hutter| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_30.pdf">http://bayesiandeeplearning.org/papers/BDL_30.pdf</a></li><li>The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables|Chris J. Maddison, Andriy Mnih and Yee Whye Teh| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_31.pdf">http://bayesiandeeplearning.org/papers/BDL_31.pdf</a></li><li>Known Unknowns: Uncertainty Quality in Bayesian Neural Networks | Ramon Oliveira, Pedro Tabacof and Eduardo Valle| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_32.pdf">http://bayesiandeeplearning.org/papers/BDL_32.pdf</a></li><li>Normalizing Flows on Riemannian Manifolds |Mevlana Gemici, Danilo Rezende and Shakir Mohamed|2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_33.pdf">http://bayesiandeeplearning.org/papers/BDL_33.pdf</a></li><li>Posterior Distribution Analysis for Bayesian Inference in Neural Networks| Pavel Myshkov and Simon Julier| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_34.pdf">http://bayesiandeeplearning.org/papers/BDL_34.pdf</a></li><li>Deep Bayesian Active Learning with Image Data| Yarin Gal, Riashat Islam and Zoubin Ghahramani| 2016<br><br>Stefan Webb| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_35.pdf">http://bayesiandeeplearning.org/papers/BDL_35.pdf</a></li><li>Bottleneck Conditional Density Estimators|Rui Shu, Hung Bui and Mohammad Ghavamzadeh| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_36.pdf">http://bayesiandeeplearning.org/papers/BDL_36.pdf</a></li><li>A Tighter Monte Carlo Objective with Renyi alpha-Divergence Measures| Stefan Webb and Yee Whye Teh| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_37.pdf">http://bayesiandeeplearning.org/papers/BDL_37.pdf</a></li><li>Bayesian Neural Networks for Predicting Learning Curves| Aaron Klein, Stefan Falkner, Jost Tobias Springenberg and Frank Hutter| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_38.pdf">http://bayesiandeeplearning.org/papers/BDL_38.pdf</a></li><li>Nested Compiled Inference for Hierarchical Reinforcement Learning|Tuan Anh Le, Atılım Güneş Baydin and Frank Wood|2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_41.pdf">http://bayesiandeeplearning.org/papers/BDL_41.pdf</a></li><li>Open Problems for Online Bayesian Inference in Neural Networks | Robert Loftin and David Roberts| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_42.pdf">http://bayesiandeeplearning.org/papers/BDL_42.pdf</a></li><li>Deep Probabilistic Programming| Dustin Tran, Matt Hoffman, Kevin Murphy, Rif Saurous, Eugene Brevdo, and David Blei| 2016<br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_43.pdf">http://bayesiandeeplearning.org/papers/BDL_43.pdf</a></li><li>Markov Chain Monte Carlo for Deep Latent Gaussian Models  |Matthew Hoffman| 2016 <br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_44.pdf">http://bayesiandeeplearning.org/papers/BDL_44.pdf</a></li><li>Semi-supervised Active Learning with Deep Probabilistic Generative Models | Amar Shah and Zoubin Ghahramani| 2016<br><br>Source: <a href="http://bayesiandeeplearning.org/papers/BDL_43.pdf">http://bayesiandeeplearning.org/papers/BDL_43.pdf</a></li><li>Thesis: Uncertainty in Deep Learning  | Yarin Gal| PhD Thesis, 2016 <br><br>Source: <a href="http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf">http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf</a>, Blog: <a href="http://mlg.eng.cam.ac.uk/yarin/blog_2248.html">http://mlg.eng.cam.ac.uk/yarin/blog_2248.html</a> <br></li><li>Deep survival analysis|R. Ranganath, A. Perotte, N. Elhadad, and D. Blei|2016 <br><br>Source: <a href="http://www.cs.columbia.edu/~blei/papers/RanganathPerotteElhadadBlei2016.pdf">http://www.cs.columbia.edu/~blei/papers/RanganathPerotteElhadadBlei2016.pdf</a></li><li>Towards Bayesian Deep Learning: A Survey| Hao Wang, Dit-Yan Yeung|2016 <br><br>Source: <a href="https://arxiv.org/pdf/1604.01662">https://arxiv.org/pdf/1604.01662</a><h4 id="2017"><a name="2017" href="#2017"></a>2017</h4>
</li><li>Dropout Inference in Bayesian Neural Networks with Alpha-divergences |Yingzhen Li, Yarin Gal|2017 <br><br>Source: <a href="https://arxiv.org/abs/1703.02914">https://arxiv.org/abs/1703.02914</a></li><li>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?  |Alex Kendall, Yarin Gal|2017 <br><br>Source: <a href="https://arxiv.org/abs/1703.04977">https://arxiv.org/abs/1703.04977</a></li></ol><h2 id="license"><a name="license" href="#license"></a>License</h2><p><a href="https://creativecommons.org/publicdomain/zero/1.0/"><img src="http://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg" alt="CC0"></a></p>

<footer style="position:fixed; font-size:.8em; text-align:right; bottom:0px; margin-left:-25px; height:20px; width:100%;">Back to :  <a href="#table-of-content">Table Of Content</a></footer>
</body>
</html>
